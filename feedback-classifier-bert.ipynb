{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import re\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from keras.layers import Input, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Model\n",
    "\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../input/final-data-combined/coments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_by_space = re.compile('[\\n/(){}\\[\\]\\|@,;]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #]')\n",
    "\n",
    "def clean(s):\n",
    "    s = s.lower()\n",
    "    s = replace_by_space.sub(' ',s)\n",
    "    s = BAD_SYMBOLS_RE.sub('', s)\n",
    "    s = ' '.join(word for word in s.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    return s\n",
    "\n",
    "\n",
    "df[\"Text\"] = df[\"Text\"].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenize the text and map words to their ids  (\"distilbert-base-uncased\", \"bert-base-uncased\")\n",
    "\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Set the maximum sequence length.\n",
    "\n",
    "MAX_LEN = 40\n",
    "\n",
    "\n",
    "tokenized = df[\"Text\"].apply(lambda s: tokenizer.encode(s,add_special_tokens=True))\n",
    "\n",
    "\n",
    "\n",
    "# Pad our input tokens with value 0.\n",
    "\n",
    "input_ids = pad_sequences(tokenized, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "\n",
    "label_cols = [\"Complaint\",\"Suggestion\",\"Compliment\"]\n",
    "labels = df[label_cols].values\n",
    "\n",
    "label_cols_complaint = [\"Complaint\"]\n",
    "label_cols_sug = [\"Suggestion\"]\n",
    "label_cols_compliment = [\"Compliment\"]\n",
    "\n",
    "labels_complaint =  df[label_cols_complaint].values\n",
    "labels_sug =  df[label_cols_sug].values\n",
    "labels_compliment =  df[label_cols_compliment].values\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_complaint, x_test_complaint, y_train_complaint, y_test_complaint = train_test_split(input_ids, labels_complaint, test_size=0.2, random_state = 62, stratify=labels_complaint)\n",
    "x_train_sug, x_test_sug, y_train_sug, y_test_sug = train_test_split(input_ids, labels_sug, test_size=0.2, random_state = 62, stratify=labels_sug)\n",
    "x_train_compliment, x_test_compliment, y_train_compliment, y_test_compliment = train_test_split(input_ids, labels_compliment, test_size=0.2, random_state = 62, stratify=labels_compliment)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(input_ids, labels, test_size=0.2, random_state = 62, stratify=labels_complaint)\n",
    "\n",
    "\n",
    "# Do the same for the masks.\n",
    "# train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,test_size=0.2, random_state = 62, stratify=labels)\n",
    "\n",
    "\n",
    "train_size = len(x_train_complaint)\n",
    "test_size = len(x_test_complaint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_models(transformer, max_len=MAX_LEN):\n",
    "    \n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    sequence_output = transformer(input_word_ids)[0]\n",
    "    cls_token = sequence_output[:, 0, :]\n",
    "    out = Dense(1, activation='sigmoid')(cls_token)\n",
    "    \n",
    "    models = [Model(inputs=input_word_ids, outputs=out), \n",
    "              Model(inputs=input_word_ids, outputs=out), Model(inputs=input_word_ids, outputs=out)]\n",
    "    for model in models:\n",
    "        model.compile(Adam(lr=2e-5), loss='binary_crossentropy', metrics=['AUC'])\n",
    "    \n",
    "    return {\"complaint\": models[0], \"suggestion\": models[1], \"compliment\":models[2] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = build_models(transformers.TFBertModel.from_pretrained('bert-base-uncased'), max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping( monitor='val_auc', verbose=2, patience=3,\n",
    "                                mode='max', restore_best_weights=True)\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\"checkpoint.h5\" \n",
    "                           ,monitor='val_auc', mode='max', verbose=2, save_best_only=True, \n",
    "                            save_weights_only=True, save_freq='epoch')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define weights for unbalanced categories\n",
    "def get_weights(y_train):\n",
    "    neg, pos = np.bincount(y_train.astype(int).flatten())\n",
    "    total = neg + pos\n",
    "\n",
    "    weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "    weight_for_1 = (1 / pos)*(total)/2.0\n",
    "\n",
    "    return weight_for_0, weight_for_1\n",
    "\n",
    "weight_for_0_complaint, weight_for_1_complaint = get_weights(y_train_complaint)\n",
    "weight_for_0_sug, weight_for_1_sug = get_weights(y_train_sug)\n",
    "weight_for_0_compliment, weight_for_1_compliment = get_weights(y_train_compliment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_complaint = models[\"complaint\"].fit(x_train_complaint, y_train_complaint , batch_size = 32 ,validation_data=(x_test_complaint,y_test_complaint),\n",
    "                     epochs = 8, class_weight={0:weight_for_0_complaint, 1:weight_for_1_complaint}, callbacks=[early_stopping,model_checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preds(model,x_test):\n",
    "    \n",
    "    model.load_weights(\"checkpoint.h5\")\n",
    "    pred = model.predict(x_test)\n",
    "    \n",
    "    return pred\n",
    "\n",
    "\n",
    "print(\"Complaint\",\"\\n\",classification_report(y_test_complaint, np.round(make_preds(models[\"complaint\"],x_test_complaint)).astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
